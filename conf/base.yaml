# ---------- common defaults shared by all runs ----------
defaults:
  - override /hydra/job_logging: disabled   # keep stdout clean

# ------------------- model settings --------------------
model:
  name: meta-llama/Meta-Llama-3-8B-Instruct
  load_4bit: true

# ------------------- LoRA settings ---------------------
lora:
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
    - up_proj
    - gate_proj
    - down_proj

# --------------- self-edit sampling --------------------
self_edits:
  samples_per_ctx: 5   # paper uses M = 5 (knowledge) or 15 (ARC)

# -------------- reinforcement loop (SEAL) -------------
rl:
  max_rounds:     4      # deeper self-learning
  reward_threshold: 0.02  # combined sim+F1 reward needed to accept
  sim_threshold:    0.02  # minimum semantic bump per edit
  f1_threshold:     0.0   # no negative F1 change allowed
  alpha:            0.7   # weight for sim vs F1 in reward

# --------------- DeepSpeed config path -----------------
deepspeed_config: ds_z3_A16.json
